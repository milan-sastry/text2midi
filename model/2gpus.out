[2025-05-06 19:12:56,728] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-06 19:13:07,152] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-06 19:13:07,155] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
CUDA_VISIBLE_DEVICES: 0,1
CUDA device count: 2
CUDA current device: 0
CUDA device name: Tesla V100-PCIE-32GB
CUDA_VISIBLE_DEVICES: 0,1
CUDA device count: 2
CUDA current device: 0
CUDA device name: Tesla V100-PCIE-32GB
/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of ðŸ¤— Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of ðŸ¤— Accelerate. Use `project_dir` instead.
  warnings.warn(
/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/accelerate/accelerator.py:416: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
05/06/2025 19:13:12 - INFO - __main__ - Distributed environment: DistributedType.MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/accelerate/accelerator.py:416: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)
05/06/2025 19:13:12 - INFO - __main__ - Distributed environment: DistributedType.MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: fp16

[rank1]:[W506 19:13:12.191500443 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
wandb: WARNING Unable to verify login in offline mode.
wandb: Tracking run with wandb version 0.19.10
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[rank0]:[W506 19:13:13.798024547 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Length of dataset:  1683
Length of dataset:  1683
/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Total number of trainable parameters: 114223628
num_update_steps_per_epoch 53
max_train_steps None
max_train_steps 7420
Total number of trainable parameters: 114223628
num_update_steps_per_epoch 53
max_train_steps None
max_train_steps 7420
/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/accelerate/accelerator.py:1235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  model.forward = torch.cuda.amp.autocast(dtype=torch.float16)(model.forward)
05/06/2025 19:13:15 - INFO - __main__ - ***** Running training *****
05/06/2025 19:13:15 - INFO - __main__ -   Num examples = 1683
05/06/2025 19:13:15 - INFO - __main__ -   Num Epochs = 140
05/06/2025 19:13:15 - INFO - __main__ -   Instantaneous batch size per device = 8
05/06/2025 19:13:15 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64
05/06/2025 19:13:15 - INFO - __main__ -   Gradient Accumulation steps = 4
05/06/2025 19:13:15 - INFO - __main__ -   Total optimization steps = 7420
/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/accelerate/accelerator.py:1235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  model.forward = torch.cuda.amp.autocast(dtype=torch.float16)(model.forward)
  0%|          | 0/7420 [00:00<?, ?it/s][rank0]:[W506 19:13:32.535415103 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W506 19:13:34.812745931 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 0/7420 [00:19<?, ?it/s, Loss=6.9]  0%|          | 1/7420 [00:19<40:06:14, 19.46s/it, Loss=6.9]  0%|          | 1/7420 [00:24<40:06:14, 19.46s/it, Loss=6.88]  0%|          | 2/7420 [00:24<22:13:18, 10.78s/it, Loss=6.88]  0%|          | 2/7420 [00:29<22:13:18, 10.78s/it, Loss=6.83]  0%|          | 3/7420 [00:29<16:49:53,  8.17s/it, Loss=6.83]  0%|          | 3/7420 [00:34<16:49:53,  8.17s/it, Loss=6.72]  0%|          | 4/7420 [00:34<14:07:28,  6.86s/it, Loss=6.72]  0%|          | 4/7420 [00:38<14:07:28,  6.86s/it, Loss=6.97]  0%|          | 5/7420 [00:38<12:25:18,  6.03s/it, Loss=6.97]  0%|          | 5/7420 [00:43<12:25:18,  6.03s/it, Loss=6.79]  0%|          | 6/7420 [00:43<11:45:44,  5.71s/it, Loss=6.79]  0%|          | 6/7420 [00:48<11:45:44,  5.71s/it, Loss=6.86]  0%|          | 7/7420 [00:48<11:12:18,  5.44s/it, Loss=6.86]  0%|          | 7/7420 [00:54<11:12:18,  5.44s/it, Loss=6.64]  0%|          | 8/7420 [00:54<11:25:35,  5.55s/it, Loss=6.64]  0%|          | 8/7420 [00:58<11:25:35,  5.55s/it, Loss=6.83]  0%|          | 9/7420 [00:58<10:38:29,  5.17s/it, Loss=6.83]  0%|          | 9/7420 [01:03<10:38:29,  5.17s/it, Loss=6.88]  0%|          | 10/7420 [01:03<10:30:28,  5.11s/it, Loss=6.88]  0%|          | 10/7420 [01:08<10:30:28,  5.11s/it, Loss=6.79]  0%|          | 11/7420 [01:08<10:24:16,  5.06s/it, Loss=6.79]  0%|          | 11/7420 [01:14<10:24:16,  5.06s/it, Loss=6.92]  0%|          | 12/7420 [01:14<10:52:27,  5.28s/it, Loss=6.92]  0%|          | 12/7420 [01:19<10:52:27,  5.28s/it, Loss=6.91]  0%|          | 13/7420 [01:19<11:00:40,  5.35s/it, Loss=6.91]  0%|          | 13/7420 [01:24<11:00:40,  5.35s/it, Loss=6.89]  0%|          | 14/7420 [01:24<10:36:59,  5.16s/it, Loss=6.89]  0%|          | 14/7420 [01:29<10:36:59,  5.16s/it, Loss=6.7]   0%|          | 15/7420 [01:29<10:42:05,  5.20s/it, Loss=6.7]  0%|          | 15/7420 [01:34<10:42:05,  5.20s/it, Loss=6.85]  0%|          | 16/7420 [01:34<10:28:42,  5.09s/it, Loss=6.85]  0%|          | 16/7420 [01:39<10:28:42,  5.09s/it, Loss=6.91]  0%|          | 17/7420 [01:39<10:26:25,  5.08s/it, Loss=6.91]  0%|          | 17/7420 [01:44<10:26:25,  5.08s/it, Loss=6.77]  0%|          | 18/7420 [01:44<10:04:38,  4.90s/it, Loss=6.77]  0%|          | 18/7420 [01:49<10:04:38,  4.90s/it, Loss=6.79]  0%|          | 19/7420 [01:49<10:13:43,  4.98s/it, Loss=6.79]  0%|          | 19/7420 [01:55<10:13:43,  4.98s/it, Loss=6.79]  0%|          | 20/7420 [01:55<10:45:38,  5.23s/it, Loss=6.79]  0%|          | 20/7420 [02:01<10:45:38,  5.23s/it, Loss=6.83]  0%|          | 21/7420 [02:01<11:22:35,  5.54s/it, Loss=6.83]  0%|          | 21/7420 [02:06<11:22:35,  5.54s/it, Loss=6.72]  0%|          | 22/7420 [02:06<11:05:17,  5.40s/it, Loss=6.72]  0%|          | 22/7420 [02:12<11:05:17,  5.40s/it, Loss=6.81]  0%|          | 23/7420 [02:12<11:19:13,  5.51s/it, Loss=6.81]  0%|          | 23/7420 [02:17<11:19:13,  5.51s/it, Loss=6.76]  0%|          | 24/7420 [02:17<10:48:26,  5.26s/it, Loss=6.76]  0%|          | 24/7420 [02:20<10:48:26,  5.26s/it, Loss=6.75]  0%|          | 25/7420 [02:20<9:26:28,  4.60s/it, Loss=6.75]   0%|          | 25/7420 [02:22<9:26:28,  4.60s/it, Loss=6.75]  0%|          | 26/7420 [02:22<8:13:16,  4.00s/it, Loss=6.75]  0%|          | 26/7420 [02:23<8:13:16,  4.00s/it, Loss=6.81]  0%|          | 27/7420 [02:23<6:13:51,  3.03s/it, Loss=6.81]Epoch: 68, Loss Train: 6.8379

05/06/2025 19:15:38 - INFO - __main__ - {'epoch': 69, 'step': 27, 'train_loss': 6.8379}
05/06/2025 19:15:38 - INFO - accelerate.accelerator - Saving current state to /scratch/network/ms1438/output_test_new/epoch_69
05/06/2025 19:15:41 - INFO - accelerate.checkpointing - Model weights saved in /scratch/network/ms1438/output_test_new/epoch_69/pytorch_model.bin
05/06/2025 19:15:43 - INFO - accelerate.checkpointing - Optimizer state saved in /scratch/network/ms1438/output_test_new/epoch_69/optimizer.bin
05/06/2025 19:15:43 - INFO - accelerate.checkpointing - Scheduler state saved in /scratch/network/ms1438/output_test_new/epoch_69/scheduler.bin
05/06/2025 19:15:43 - INFO - accelerate.checkpointing - Gradient scaler state saved in /scratch/network/ms1438/output_test_new/epoch_69/scaler.pt
05/06/2025 19:15:43 - INFO - accelerate.checkpointing - Random states saved in /scratch/network/ms1438/output_test_new/epoch_69/random_states_0.pkl
  0%|          | 27/7420 [02:47<6:13:51,  3.03s/it, Loss=6.77]  0%|          | 28/7420 [02:47<19:01:30,  9.27s/it, Loss=6.77]  0%|          | 28/7420 [02:53<19:01:30,  9.27s/it, Loss=6.83]  0%|          | 29/7420 [02:53<17:13:53,  8.39s/it, Loss=6.83]  0%|          | 29/7420 [02:58<17:13:53,  8.39s/it, Loss=6.56]  0%|          | 30/7420 [02:58<15:03:15,  7.33s/it, Loss=6.56]  0%|          | 30/7420 [03:04<15:03:15,  7.33s/it, Loss=6.76]  0%|          | 31/7420 [03:04<14:07:23,  6.88s/it, Loss=6.76]  0%|          | 31/7420 [03:09<14:07:23,  6.88s/it, Loss=6.74]  0%|          | 32/7420 [03:09<13:07:01,  6.39s/it, Loss=6.74]  0%|          | 32/7420 [03:14<13:07:01,  6.39s/it, Loss=6.75]  0%|          | 33/7420 [03:14<12:16:30,  5.98s/it, Loss=6.75]  0%|          | 33/7420 [03:19<12:16:30,  5.98s/it, Loss=6.77]  0%|          | 34/7420 [03:19<11:33:16,  5.63s/it, Loss=6.77]  0%|          | 34/7420 [03:24<11:33:16,  5.63s/it, Loss=6.8]   0%|          | 35/7420 [03:24<11:13:28,  5.47s/it, Loss=6.8]  0%|          | 35/7420 [03:29<11:13:28,  5.47s/it, Loss=6.82]  0%|          | 36/7420 [03:29<11:04:11,  5.40s/it, Loss=6.82]  0%|          | 36/7420 [03:34<11:04:11,  5.40s/it, Loss=6.8]   0%|          | 37/7420 [03:34<10:38:46,  5.19s/it, Loss=6.8]  0%|          | 37/7420 [03:39<10:38:46,  5.19s/it, Loss=6.62]  1%|          | 38/7420 [03:39<10:30:43,  5.13s/it, Loss=6.62]  1%|          | 38/7420 [03:44<10:30:43,  5.13s/it, Loss=6.7]   1%|          | 39/7420 [03:44<10:32:12,  5.14s/it, Loss=6.7]  1%|          | 39/7420 [03:49<10:32:12,  5.14s/it, Loss=6.74]  1%|          | 40/7420 [03:49<10:37:55,  5.19s/it, Loss=6.74]  1%|          | 40/7420 [03:55<10:37:55,  5.19s/it, Loss=6.66]  1%|          | 41/7420 [03:55<10:40:43,  5.21s/it, Loss=6.66]  1%|          | 41/7420 [04:00<10:40:43,  5.21s/it, Loss=6.68]  1%|          | 42/7420 [04:00<10:28:57,  5.11s/it, Loss=6.68]  1%|          | 42/7420 [04:06<10:28:57,  5.11s/it, Loss=6.66]  1%|          | 43/7420 [04:06<11:16:40,  5.50s/it, Loss=6.66]  1%|          | 43/7420 [04:12<11:16:40,  5.50s/it, Loss=6.69]  1%|          | 44/7420 [04:12<11:18:50,  5.52s/it, Loss=6.69]  1%|          | 44/7420 [04:18<11:18:50,  5.52s/it, Loss=6.72]  1%|          | 45/7420 [04:18<11:34:46,  5.65s/it, Loss=6.72]  1%|          | 45/7420 [04:23<11:34:46,  5.65s/it, Loss=6.57]  1%|          | 46/7420 [04:23<11:27:33,  5.59s/it, Loss=6.57]  1%|          | 46/7420 [04:28<11:27:33,  5.59s/it, Loss=6.51]  1%|          | 47/7420 [04:28<11:03:59,  5.40s/it, Loss=6.51]  1%|          | 47/7420 [04:33<11:03:59,  5.40s/it, Loss=6.73]  1%|          | 48/7420 [04:33<11:01:23,  5.38s/it, Loss=6.73]  1%|          | 48/7420 [04:37<11:01:23,  5.38s/it, Loss=6.72]  1%|          | 49/7420 [04:37<10:14:24,  5.00s/it, Loss=6.72]  1%|          | 49/7420 [04:43<10:14:24,  5.00s/it, Loss=6.55]  1%|          | 50/7420 [04:43<10:20:26,  5.05s/it, Loss=6.55]  1%|          | 50/7420 [04:48<10:20:26,  5.05s/it, Loss=6.62]  1%|          | 51/7420 [04:48<10:37:38,  5.19s/it, Loss=6.62]  1%|          | 51/7420 [04:51<10:37:38,  5.19s/it, Loss=6.75]  1%|          | 52/7420 [04:51<9:17:49,  4.54s/it, Loss=6.75]   1%|          | 52/7420 [04:54<9:17:49,  4.54s/it, Loss=6.57]  1%|          | 53/7420 [04:54<8:03:57,  3.94s/it, Loss=6.57]  1%|          | 53/7420 [04:55<8:03:57,  3.94s/it, Loss=6.58]  1%|          | 54/7420 [04:55<6:12:21,  3.03s/it, Loss=6.58]Epoch: 69, Loss Train: 6.6902

05/06/2025 19:18:10 - INFO - __main__ - {'epoch': 70, 'step': 54, 'train_loss': 6.6902}
05/06/2025 19:18:10 - INFO - accelerate.accelerator - Saving current state to /scratch/network/ms1438/output_test_new/epoch_70
05/06/2025 19:18:12 - INFO - accelerate.checkpointing - Model weights saved in /scratch/network/ms1438/output_test_new/epoch_70/pytorch_model.bin
05/06/2025 19:18:14 - INFO - accelerate.checkpointing - Optimizer state saved in /scratch/network/ms1438/output_test_new/epoch_70/optimizer.bin
05/06/2025 19:18:14 - INFO - accelerate.checkpointing - Scheduler state saved in /scratch/network/ms1438/output_test_new/epoch_70/scheduler.bin
05/06/2025 19:18:14 - INFO - accelerate.checkpointing - Gradient scaler state saved in /scratch/network/ms1438/output_test_new/epoch_70/scaler.pt
05/06/2025 19:18:15 - INFO - accelerate.checkpointing - Random states saved in /scratch/network/ms1438/output_test_new/epoch_70/random_states_0.pkl
  1%|          | 54/7420 [05:18<6:12:21,  3.03s/it, Loss=6.58]  1%|          | 55/7420 [05:18<18:42:28,  9.14s/it, Loss=6.58]  1%|          | 55/7420 [05:23<18:42:28,  9.14s/it, Loss=6.56]  1%|          | 56/7420 [05:23<16:02:02,  7.84s/it, Loss=6.56]  1%|          | 56/7420 [05:28<16:02:02,  7.84s/it, Loss=6.57]  1%|          | 57/7420 [05:28<14:26:33,  7.06s/it, Loss=6.57]  1%|          | 57/7420 [05:34<14:26:33,  7.06s/it, Loss=6.5]   1%|          | 58/7420 [05:34<13:43:36,  6.71s/it, Loss=6.5]  1%|          | 58/7420 [05:39<13:43:36,  6.71s/it, Loss=6.62]  1%|          | 59/7420 [05:39<12:30:51,  6.12s/it, Loss=6.62]  1%|          | 59/7420 [05:44<12:30:51,  6.12s/it, Loss=6.45]  1%|          | 60/7420 [05:44<12:02:56,  5.89s/it, Loss=6.45]  1%|          | 60/7420 [05:49<12:02:56,  5.89s/it, Loss=6.4]   1%|          | 61/7420 [05:49<11:18:07,  5.53s/it, Loss=6.4]  1%|          | 61/7420 [05:54<11:18:07,  5.53s/it, Loss=6.34]  1%|          | 62/7420 [05:54<11:07:36,  5.44s/it, Loss=6.34]  1%|          | 62/7420 [05:59<11:07:36,  5.44s/it, Loss=6.35]  1%|          | 63/7420 [05:59<10:40:54,  5.23s/it, Loss=6.35]  1%|          | 63/7420 [06:04<10:40:54,  5.23s/it, Loss=6.42]  1%|          | 64/7420 [06:04<10:29:50,  5.14s/it, Loss=6.42]  1%|          | 64/7420 [06:09<10:29:50,  5.14s/it, Loss=6.51]  1%|          | 65/7420 [06:09<10:47:19,  5.28s/it, Loss=6.51]  1%|          | 65/7420 [06:14<10:47:19,  5.28s/it, Loss=6.42]  1%|          | 66/7420 [06:14<10:28:11,  5.13s/it, Loss=6.42]  1%|          | 66/7420 [06:19<10:28:11,  5.13s/it, Loss=6.44]  1%|          | 67/7420 [06:19<10:13:47,  5.01s/it, Loss=6.44]  1%|          | 67/7420 [06:24<10:13:47,  5.01s/it, Loss=6.32]  1%|          | 68/7420 [06:24<10:08:57,  4.97s/it, Loss=6.32]  1%|          | 68/7420 [06:28<10:08:57,  4.97s/it, Loss=6.35]  1%|          | 69/7420 [06:28<9:48:33,  4.80s/it, Loss=6.35]   1%|          | 69/7420 [06:33<9:48:33,  4.80s/it, Loss=5.94]  1%|          | 70/7420 [06:33<9:41:09,  4.74s/it, Loss=5.94]  1%|          | 70/7420 [06:38<9:41:09,  4.74s/it, Loss=6.43]  1%|          | 71/7420 [06:38<10:08:29,  4.97s/it, Loss=6.43]  1%|          | 71/7420 [06:44<10:08:29,  4.97s/it, Loss=6.27]  1%|          | 72/7420 [06:44<10:28:00,  5.13s/it, Loss=6.27]  1%|          | 72/7420 [06:49<10:28:00,  5.13s/it, Loss=6.25]  1%|          | 73/7420 [06:49<10:36:26,  5.20s/it, Loss=6.25]  1%|          | 73/7420 [06:55<10:36:26,  5.20s/it, Loss=6.33]  1%|          | 74/7420 [06:55<10:52:09,  5.33s/it, Loss=6.33]  1%|          | 74/7420 [07:00<10:52:09,  5.33s/it, Loss=6.15]  1%|          | 75/7420 [07:00<10:56:51,  5.37s/it, Loss=6.15]  1%|          | 75/7420 [07:05<10:56:51,  5.37s/it, Loss=6.5]   1%|          | 76/7420 [07:05<10:42:11,  5.25s/it, Loss=6.5]  1%|          | 76/7420 [07:10<10:42:11,  5.25s/it, Loss=6.24]  1%|          | 77/7420 [07:10<10:45:39,  5.28s/it, Loss=6.24]  1%|          | 77/7420 [07:15<10:45:39,  5.28s/it, Loss=6.32]  1%|          | 78/7420 [07:15<10:38:06,  5.21s/it, Loss=6.32]  1%|          | 78/7420 [07:18<10:38:06,  5.21s/it, Loss=5.95]  1%|          | 79/7420 [07:18<9:12:34,  4.52s/it, Loss=5.95]   1%|          | 79/7420 [07:21<9:12:34,  4.52s/it, Loss=6.26]  1%|          | 80/7420 [07:21<8:02:03,  3.94s/it, Loss=6.26]  1%|          | 80/7420 [07:22<8:02:03,  3.94s/it, Loss=6]     1%|          | 81/7420 [07:22<6:07:49,  3.01s/it, Loss=6]Epoch: 70, Loss Train: 6.3716

05/06/2025 19:20:37 - INFO - __main__ - {'epoch': 71, 'step': 81, 'train_loss': 6.3716}
05/06/2025 19:20:37 - INFO - accelerate.accelerator - Saving current state to /scratch/network/ms1438/output_test_new/epoch_71
05/06/2025 19:20:39 - INFO - accelerate.checkpointing - Model weights saved in /scratch/network/ms1438/output_test_new/epoch_71/pytorch_model.bin
05/06/2025 19:20:41 - INFO - accelerate.checkpointing - Optimizer state saved in /scratch/network/ms1438/output_test_new/epoch_71/optimizer.bin
05/06/2025 19:20:42 - INFO - accelerate.checkpointing - Scheduler state saved in /scratch/network/ms1438/output_test_new/epoch_71/scheduler.bin
05/06/2025 19:20:42 - INFO - accelerate.checkpointing - Gradient scaler state saved in /scratch/network/ms1438/output_test_new/epoch_71/scaler.pt
05/06/2025 19:20:42 - INFO - accelerate.checkpointing - Random states saved in /scratch/network/ms1438/output_test_new/epoch_71/random_states_0.pkl
  1%|          | 81/7420 [07:45<6:07:49,  3.01s/it, Loss=6.26]  1%|          | 82/7420 [07:45<18:42:22,  9.18s/it, Loss=6.26]  1%|          | 82/7420 [07:50<18:42:22,  9.18s/it, Loss=6.13]  1%|          | 83/7420 [07:50<16:08:22,  7.92s/it, Loss=6.13]  1%|          | 83/7420 [07:55<16:08:22,  7.92s/it, Loss=6.19]  1%|          | 84/7420 [07:55<13:58:39,  6.86s/it, Loss=6.19]  1%|          | 84/7420 [08:00<13:58:39,  6.86s/it, Loss=5.92]  1%|          | 85/7420 [08:00<13:05:23,  6.42s/it, Loss=5.92]  1%|          | 85/7420 [08:05<13:05:23,  6.42s/it, Loss=6.16]  1%|          | 86/7420 [08:05<12:26:43,  6.11s/it, Loss=6.16]  1%|          | 86/7420 [08:11<12:26:43,  6.11s/it, Loss=5.92]  1%|          | 87/7420 [08:11<11:57:14,  5.87s/it, Loss=5.92]  1%|          | 87/7420 [08:17<11:57:14,  5.87s/it, Loss=5.7]   1%|          | 88/7420 [08:17<11:55:30,  5.86s/it, Loss=5.7]  1%|          | 88/7420 [08:21<11:55:30,  5.86s/it, Loss=6.24]  1%|          | 89/7420 [08:21<10:50:30,  5.32s/it, Loss=6.24]  1%|          | 89/7420 [08:26<10:50:30,  5.32s/it, Loss=6.16]  1%|          | 90/7420 [08:26<10:51:28,  5.33s/it, Loss=6.16]  1%|          | 90/7420 [08:31<10:51:28,  5.33s/it, Loss=6.34]  1%|          | 91/7420 [08:31<10:40:48,  5.25s/it, Loss=6.34]  1%|          | 91/7420 [08:36<10:40:48,  5.25s/it, Loss=6.2]   1%|          | 92/7420 [08:36<10:43:23,  5.27s/it, Loss=6.2]  1%|          | 92/7420 [08:42<10:43:23,  5.27s/it, Loss=6.08]  1%|â–         | 93/7420 [08:42<11:02:53,  5.43s/it, Loss=6.08]  1%|â–         | 93/7420 [08:47<11:02:53,  5.43s/it, Loss=6.26]  1%|â–         | 94/7420 [08:47<10:32:44,  5.18s/it, Loss=6.26]  1%|â–         | 94/7420 [08:51<10:32:44,  5.18s/it, Loss=6.07]  1%|â–         | 95/7420 [08:51<10:03:43,  4.95s/it, Loss=6.07]  1%|â–         | 95/7420 [08:57<10:03:43,  4.95s/it, Loss=6.24]  1%|â–         | 96/7420 [08:57<10:22:51,  5.10s/it, Loss=6.24]  1%|â–         | 96/7420 [09:03<10:22:51,  5.10s/it, Loss=6.14]  1%|â–         | 97/7420 [09:03<11:22:50,  5.59s/it, Loss=6.14]  1%|â–         | 97/7420 [09:08<11:22:50,  5.59s/it, Loss=6.16]  1%|â–         | 98/7420 [09:08<10:59:56,  5.41s/it, Loss=6.16]  1%|â–         | 98/7420 [09:13<10:59:56,  5.41s/it, Loss=4.96]  1%|â–         | 99/7420 [09:13<10:28:29,  5.15s/it, Loss=4.96]  1%|â–         | 99/7420 [09:18<10:28:29,  5.15s/it, Loss=6.2]   1%|â–         | 100/7420 [09:18<10:27:30,  5.14s/it, Loss=6.2]  1%|â–         | 100/7420 [09:23<10:27:30,  5.14s/it, Loss=5.56]  1%|â–         | 101/7420 [09:23<10:36:16,  5.22s/it, Loss=5.56]  1%|â–         | 101/7420 [09:28<10:36:16,  5.22s/it, Loss=5.83]  1%|â–         | 102/7420 [09:28<10:21:39,  5.10s/it, Loss=5.83]  1%|â–         | 102/7420 [09:35<10:21:39,  5.10s/it, Loss=6.07]  1%|â–         | 103/7420 [09:35<11:06:24,  5.46s/it, Loss=6.07]  1%|â–         | 103/7420 [09:39<11:06:24,  5.46s/it, Loss=5.73]  1%|â–         | 104/7420 [09:39<10:30:17,  5.17s/it, Loss=5.73]  1%|â–         | 104/7420 [09:44<10:30:17,  5.17s/it, Loss=6.14]  1%|â–         | 105/7420 [09:44<10:31:46,  5.18s/it, Loss=6.14]  1%|â–         | 105/7420 [09:47<10:31:46,  5.18s/it, Loss=5.69]  1%|â–         | 106/7420 [09:47<9:06:48,  4.49s/it, Loss=5.69]   1%|â–         | 106/7420 [09:50<9:06:48,  4.49s/it, Loss=5.51]  1%|â–         | 107/7420 [09:50<7:57:02,  3.91s/it, Loss=5.51]  1%|â–         | 107/7420 [09:51<7:57:02,  3.91s/it, Loss=5.48]  1%|â–         | 108/7420 [09:51<6:06:58,  3.01s/it, Loss=5.48]Epoch: 71, Loss Train: 5.9788

05/06/2025 19:23:06 - INFO - __main__ - {'epoch': 72, 'step': 108, 'train_loss': 5.9788}
05/06/2025 19:23:06 - INFO - accelerate.accelerator - Saving current state to /scratch/network/ms1438/output_test_new/epoch_72
05/06/2025 19:23:08 - INFO - accelerate.checkpointing - Model weights saved in /scratch/network/ms1438/output_test_new/epoch_72/pytorch_model.bin
05/06/2025 19:23:10 - INFO - accelerate.checkpointing - Optimizer state saved in /scratch/network/ms1438/output_test_new/epoch_72/optimizer.bin
05/06/2025 19:23:10 - INFO - accelerate.checkpointing - Scheduler state saved in /scratch/network/ms1438/output_test_new/epoch_72/scheduler.bin
05/06/2025 19:23:10 - INFO - accelerate.checkpointing - Gradient scaler state saved in /scratch/network/ms1438/output_test_new/epoch_72/scaler.pt
05/06/2025 19:23:11 - INFO - accelerate.checkpointing - Random states saved in /scratch/network/ms1438/output_test_new/epoch_72/random_states_0.pkl
  1%|â–         | 108/7420 [10:18<6:06:58,  3.01s/it, Loss=6.24]  1%|â–         | 109/7420 [10:18<20:47:09, 10.24s/it, Loss=6.24]  1%|â–         | 109/7420 [10:24<20:47:09, 10.24s/it, Loss=5.64]  1%|â–         | 110/7420 [10:24<18:35:02,  9.15s/it, Loss=5.64]  1%|â–         | 110/7420 [10:29<18:35:02,  9.15s/it, Loss=5.06]  1%|â–         | 111/7420 [10:29<15:56:22,  7.85s/it, Loss=5.06]  1%|â–         | 111/7420 [10:36<15:56:22,  7.85s/it, Loss=5.12]  2%|â–         | 112/7420 [10:36<15:03:21,  7.42s/it, Loss=5.12]  2%|â–         | 112/7420 [10:40<15:03:21,  7.42s/it, Loss=5.95]  2%|â–         | 113/7420 [10:40<13:06:33,  6.46s/it, Loss=5.95]  2%|â–         | 113/7420 [10:46<13:06:33,  6.46s/it, Loss=5.32]  2%|â–         | 114/7420 [10:46<12:44:12,  6.28s/it, Loss=5.32]  2%|â–         | 114/7420 [10:52<12:44:12,  6.28s/it, Loss=6]     2%|â–         | 115/7420 [10:52<12:42:44,  6.26s/it, Loss=6]  2%|â–         | 115/7420 [10:57<12:42:44,  6.26s/it, Loss=5.5]  2%|â–         | 116/7420 [10:57<12:18:54,  6.07s/it, Loss=5.5]  2%|â–         | 116/7420 [11:03<12:18:54,  6.07s/it, Loss=6]    2%|â–         | 117/7420 [11:03<12:09:18,  5.99s/it, Loss=6]  2%|â–         | 117/7420 [11:10<12:09:18,  5.99s/it, Loss=5.98]  2%|â–         | 118/7420 [11:10<12:27:01,  6.14s/it, Loss=5.98]  2%|â–         | 118/7420 [11:15<12:27:01,  6.14s/it, Loss=5.4]   2%|â–         | 119/7420 [11:15<11:48:57,  5.83s/it, Loss=5.4]  2%|â–         | 119/7420 [11:21<11:48:57,  5.83s/it, Loss=5.48]  2%|â–         | 120/7420 [11:21<11:59:59,  5.92s/it, Loss=5.48]  2%|â–         | 120/7420 [11:27<11:59:59,  5.92s/it, Loss=5.84]  2%|â–         | 121/7420 [11:27<12:17:33,  6.06s/it, Loss=5.84]  2%|â–         | 121/7420 [11:32<12:17:33,  6.06s/it, Loss=5.41]  2%|â–         | 122/7420 [11:32<11:31:00,  5.68s/it, Loss=5.41]  2%|â–         | 122/7420 [11:38<11:31:00,  5.68s/it, Loss=5.57]  2%|â–         | 123/7420 [11:38<11:22:19,  5.61s/it, Loss=5.57]  2%|â–         | 123/7420 [11:44<11:22:19,  5.61s/it, Loss=5.98]  2%|â–         | 124/7420 [11:44<11:36:31,  5.73s/it, Loss=5.98]  2%|â–         | 124/7420 [11:52<11:36:31,  5.73s/it, Loss=5.76]  2%|â–         | 125/7420 [11:52<13:13:56,  6.53s/it, Loss=5.76]  2%|â–         | 125/7420 [11:58<13:13:56,  6.53s/it, Loss=5.75]  2%|â–         | 126/7420 [11:58<12:39:44,  6.25s/it, Loss=5.75]  2%|â–         | 126/7420 [12:04<12:39:44,  6.25s/it, Loss=5.71]  2%|â–         | 127/7420 [12:04<12:38:13,  6.24s/it, Loss=5.71]  2%|â–         | 127/7420 [12:10<12:38:13,  6.24s/it, Loss=5.88]  2%|â–         | 128/7420 [12:10<12:37:18,  6.23s/it, Loss=5.88]  2%|â–         | 128/7420 [12:16<12:37:18,  6.23s/it, Loss=5.86]  2%|â–         | 129/7420 [12:16<12:17:56,  6.07s/it, Loss=5.86]  2%|â–         | 129/7420 [12:21<12:17:56,  6.07s/it, Loss=5.87]  2%|â–         | 130/7420 [12:21<11:55:59,  5.89s/it, Loss=5.87]  2%|â–         | 130/7420 [12:28<11:55:59,  5.89s/it, Loss=5.54]  2%|â–         | 131/7420 [12:28<12:32:42,  6.20s/it, Loss=5.54]  2%|â–         | 131/7420 [12:35<12:32:42,  6.20s/it, Loss=5.26]  2%|â–         | 132/7420 [12:35<12:42:09,  6.27s/it, Loss=5.26]  2%|â–         | 132/7420 [12:38<12:42:09,  6.27s/it, Loss=5.84]  2%|â–         | 133/7420 [12:38<10:49:50,  5.35s/it, Loss=5.84]  2%|â–         | 133/7420 [12:40<10:49:50,  5.35s/it, Loss=5.84]  2%|â–         | 134/7420 [12:40<9:12:55,  4.55s/it, Loss=5.84]   2%|â–         | 134/7420 [12:41<9:12:55,  4.55s/it, Loss=5.79]  2%|â–         | 135/7420 [12:41<6:56:53,  3.43s/it, Loss=5.79]Epoch: 72, Loss Train: 5.6851

05/06/2025 19:25:57 - INFO - __main__ - {'epoch': 73, 'step': 135, 'train_loss': 5.6851}
05/06/2025 19:25:57 - INFO - accelerate.accelerator - Saving current state to /scratch/network/ms1438/output_test_new/epoch_73
05/06/2025 19:25:59 - INFO - accelerate.checkpointing - Model weights saved in /scratch/network/ms1438/output_test_new/epoch_73/pytorch_model.bin
05/06/2025 19:26:02 - INFO - accelerate.checkpointing - Optimizer state saved in /scratch/network/ms1438/output_test_new/epoch_73/optimizer.bin
05/06/2025 19:26:02 - INFO - accelerate.checkpointing - Scheduler state saved in /scratch/network/ms1438/output_test_new/epoch_73/scheduler.bin
05/06/2025 19:26:02 - INFO - accelerate.checkpointing - Gradient scaler state saved in /scratch/network/ms1438/output_test_new/epoch_73/scaler.pt
05/06/2025 19:26:02 - INFO - accelerate.checkpointing - Random states saved in /scratch/network/ms1438/output_test_new/epoch_73/random_states_0.pkl
  2%|â–         | 135/7420 [13:10<6:56:53,  3.43s/it, Loss=5.11]  2%|â–         | 136/7420 [13:10<22:21:58, 11.05s/it, Loss=5.11]  2%|â–         | 136/7420 [13:18<22:21:58, 11.05s/it, Loss=5.67]  2%|â–         | 137/7420 [13:18<20:30:13, 10.14s/it, Loss=5.67]  2%|â–         | 137/7420 [13:24<20:30:13, 10.14s/it, Loss=5.77]  2%|â–         | 138/7420 [13:24<17:42:19,  8.75s/it, Loss=5.77]  2%|â–         | 138/7420 [13:30<17:42:19,  8.75s/it, Loss=5.7]   2%|â–         | 139/7420 [13:30<16:28:40,  8.15s/it, Loss=5.7]  2%|â–         | 139/7420 [13:35<16:28:40,  8.15s/it, Loss=5.3]  2%|â–         | 140/7420 [13:35<14:35:11,  7.21s/it, Loss=5.3]  2%|â–         | 140/7420 [13:42<14:35:11,  7.21s/it, Loss=4.7]  2%|â–         | 141/7420 [13:42<13:59:17,  6.92s/it, Loss=4.7]  2%|â–         | 141/7420 [13:49<13:59:17,  6.92s/it, Loss=5.65]  2%|â–         | 142/7420 [13:49<14:17:21,  7.07s/it, Loss=5.65]  2%|â–         | 142/7420 [13:56<14:17:21,  7.07s/it, Loss=5.74]  2%|â–         | 143/7420 [13:56<14:04:53,  6.97s/it, Loss=5.74]  2%|â–         | 143/7420 [14:01<14:04:53,  6.97s/it, Loss=5.67]  2%|â–         | 144/7420 [14:01<13:16:55,  6.57s/it, Loss=5.67]  2%|â–         | 144/7420 [14:09<13:16:55,  6.57s/it, Loss=5.62]  2%|â–         | 145/7420 [14:09<14:07:30,  6.99s/it, Loss=5.62]  2%|â–         | 145/7420 [14:14<14:07:30,  6.99s/it, Loss=5.59]  2%|â–         | 146/7420 [14:14<12:51:18,  6.36s/it, Loss=5.59]  2%|â–         | 146/7420 [14:20<12:51:18,  6.36s/it, Loss=5.63]  2%|â–         | 147/7420 [14:20<12:32:04,  6.20s/it, Loss=5.63]  2%|â–         | 147/7420 [14:27<12:32:04,  6.20s/it, Loss=4.27]  2%|â–         | 148/7420 [14:27<13:07:29,  6.50s/it, Loss=4.27]  2%|â–         | 148/7420 [14:33<13:07:29,  6.50s/it, Loss=4.69]  2%|â–         | 149/7420 [14:33<12:46:05,  6.32s/it, Loss=4.69]  2%|â–         | 149/7420 [14:40<12:46:05,  6.32s/it, Loss=5.1]   2%|â–         | 150/7420 [14:40<12:49:20,  6.35s/it, Loss=5.1]  2%|â–         | 150/7420 [14:46<12:49:20,  6.35s/it, Loss=5.71]  2%|â–         | 151/7420 [14:46<12:53:06,  6.38s/it, Loss=5.71]  2%|â–         | 151/7420 [14:52<12:53:06,  6.38s/it, Loss=5.1]   2%|â–         | 152/7420 [14:52<12:42:17,  6.29s/it, Loss=5.1]slurmstepd: error: *** JOB 2463365 ON adroit-h11g3 CANCELLED AT 2025-05-06T19:28:08 DUE TO TIME LIMIT ***
