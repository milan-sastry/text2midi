[2025-05-06 16:30:37,541] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/home/ms1438/.conda/envs/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Vocab size:  524
Length of dataset:  1683
Device: cuda:0
Number of parameters: 225425036
Number of trainable parameters: 114223628
Epoch 1/140:   0%|          | 0/1683 [00:00<?, ?it/s]Epoch 1/140:   0%|          | 0/1683 [00:05<?, ?it/s, Loss=7.06]Epoch 1/140:   0%|          | 1/1683 [00:05<2:23:33,  5.12s/it, Loss=7.06]Epoch 1/140:   0%|          | 1/1683 [00:20<2:23:33,  5.12s/it, Loss=6.66]Epoch 1/140:   0%|          | 2/1683 [00:20<5:13:21, 11.18s/it, Loss=6.66]Epoch 1/140:   0%|          | 2/1683 [00:36<5:13:21, 11.18s/it, Loss=5.56]Epoch 1/140:   0%|          | 3/1683 [00:36<6:07:58, 13.14s/it, Loss=5.56]Epoch 1/140:   0%|          | 3/1683 [00:51<6:07:58, 13.14s/it, Loss=5.26]Epoch 1/140:   0%|          | 4/1683 [00:51<6:37:47, 14.22s/it, Loss=5.26]Epoch 1/140:   0%|          | 4/1683 [01:07<6:37:47, 14.22s/it, Loss=5.06]Epoch 1/140:   0%|          | 5/1683 [01:07<6:47:14, 14.56s/it, Loss=5.06]Epoch 1/140:   0%|          | 5/1683 [01:22<6:47:14, 14.56s/it, Loss=4.96]Epoch 1/140:   0%|          | 6/1683 [01:22<6:51:20, 14.72s/it, Loss=4.96]Epoch 1/140:   0%|          | 6/1683 [01:37<6:51:20, 14.72s/it, Loss=5.18]Epoch 1/140:   0%|          | 7/1683 [01:37<6:55:37, 14.88s/it, Loss=5.18]Epoch 1/140:   0%|          | 7/1683 [01:52<6:55:37, 14.88s/it, Loss=5.09]Epoch 1/140:   0%|          | 8/1683 [01:52<6:54:55, 14.86s/it, Loss=5.09]Epoch 1/140:   0%|          | 8/1683 [02:07<6:54:55, 14.86s/it, Loss=4.63]Epoch 1/140:   1%|          | 9/1683 [02:07<7:01:21, 15.10s/it, Loss=4.63]Epoch 1/140:   1%|          | 9/1683 [02:22<7:01:21, 15.10s/it, Loss=5.03]Epoch 1/140:   1%|          | 10/1683 [02:22<6:59:09, 15.03s/it, Loss=5.03]Epoch 1/140:   1%|          | 10/1683 [02:37<6:59:09, 15.03s/it, Loss=5.11]Epoch 1/140:   1%|          | 11/1683 [02:37<6:58:19, 15.01s/it, Loss=5.11]Epoch 1/140:   1%|          | 11/1683 [02:52<6:58:19, 15.01s/it, Loss=5.02]Epoch 1/140:   1%|          | 12/1683 [02:52<7:00:28, 15.10s/it, Loss=5.02]Epoch 1/140:   1%|          | 12/1683 [03:08<7:00:28, 15.10s/it, Loss=5.42]Epoch 1/140:   1%|          | 13/1683 [03:08<7:01:29, 15.14s/it, Loss=5.42]Epoch 1/140:   1%|          | 13/1683 [03:23<7:01:29, 15.14s/it, Loss=4.79]Epoch 1/140:   1%|          | 14/1683 [03:23<7:04:49, 15.27s/it, Loss=4.79]Epoch 1/140:   1%|          | 14/1683 [03:38<7:04:49, 15.27s/it, Loss=5.57]Epoch 1/140:   1%|          | 15/1683 [03:38<7:02:25, 15.20s/it, Loss=5.57]Epoch 1/140:   1%|          | 15/1683 [03:53<7:02:25, 15.20s/it, Loss=4.88]Epoch 1/140:   1%|          | 16/1683 [03:53<7:02:08, 15.19s/it, Loss=4.88]Epoch 1/140:   1%|          | 16/1683 [04:08<7:02:08, 15.19s/it, Loss=5.25]Epoch 1/140:   1%|          | 17/1683 [04:08<6:59:12, 15.10s/it, Loss=5.25]Epoch 1/140:   1%|          | 17/1683 [04:23<6:59:12, 15.10s/it, Loss=5.36]Epoch 1/140:   1%|          | 18/1683 [04:23<6:57:12, 15.03s/it, Loss=5.36]Epoch 1/140:   1%|          | 18/1683 [04:38<6:57:12, 15.03s/it, Loss=6.24]Epoch 1/140:   1%|          | 19/1683 [04:38<6:57:37, 15.06s/it, Loss=6.24]slurmstepd: error: *** JOB 2462802 ON adroit-h11g3 CANCELLED AT 2025-05-06T16:35:38 DUE TO TIME LIMIT ***
